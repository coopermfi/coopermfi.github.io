 <!-- FlatFy Theme - Andrea Galanti /-->
<!doctype html>
<!--[if lt IE 7]> <html class="no-js ie6 oldie" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js ie7 oldie" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js ie8 oldie" lang="en"> <![endif]-->
<!--[if IE 9]>    <html class="no-js ie9" lang="en"> <![endif]-->
<!--[if gt IE 9]><!--> <html> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0">
    <meta name="description" content="First Cooperative Perception Workshop  ">
    <meta name="author" content="">

    <title>Cooperative Perception Workshop 2022</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
 
    <!-- Custom Google Web Font -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Lato:100,300,400,700,900,100italic,300italic,400italic,700italic,900italic' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Arvo:400,700' rel='stylesheet' type='text/css'>
	
    <!-- Custom CSS-->
    <link href="css/general.css" rel="stylesheet">
	
	 <!-- Owl-Carousel -->
    <link href="css/custom.css" rel="stylesheet">
	<link href="css/owl.carousel.css" rel="stylesheet">
    <link href="css/owl.theme.css" rel="stylesheet">
	<link href="css/style.css" rel="stylesheet">
	<link href="css/animate.css" rel="stylesheet">
	
	<!-- Magnific Popup core CSS file -->
	<link rel="stylesheet" href="css/magnific-popup.css"> 
	
	<script src="js/modernizr-2.8.3.min.js"></script>  <!-- Modernizr /-->
	<!--[if IE 9]>
		<script src="js/PIE_IE9.js"></script>
	<![endif]-->
	<!--[if lt IE 9]>
		<script src="js/PIE_IE678.js"></script>
	<![endif]-->

	<!--[if lt IE 9]>
		<script src="js/html5shiv.js"></script>
	<![endif]-->

</head>

<body id="home">

	<!-- Preloader -->
	<div id="preloader">
		<div id="status"></div>
	</div>
	
	<!-- FullScreen -->
    <div class="intro-header">
		<div class="col-xs-12 text-center abcen1">
			<h1 class="h1_home wow fadeIn" data-wow-delay="0.4s" style="text-shadow: 0 0 8px #000000;">1<sup>st</sup>  Cooperative Perception Workshop </h1>
			<h3 class="h3_home wow fadeIn" data-wow-delay="0.6s" style="text-shadow: 0px 0px 4px black, 0 0 25px black"> 
				<br></br>
				<br></br>
				<p>In conjunction with <a style="color:white" href="https://www.mfi2022.com/" target="_blank"><b>mfi 2022</b></a>. </p> September 22<sup>nd</sup> 2022 (Afternoon)</p>
				<!-- <p>Room: Seaside 7</p> -->
			</h3>
			<!--ul class="list-inline intro-social-buttons">
				<li><a href="https://twitter.com/galantiandrea" class="btn  btn-lg mybutton_cyano wow fadeIn" data-wow-delay="0.8s"><span class="network-name">Twitter</span></a>
				</li>
				<li id="download" ><a href="#downloadlink" class="btn  btn-lg mybutton_standard wow swing wow fadeIn" data-wow-delay="1.2s"><span class="network-name">Free Download</span></a>
				</li>
			</ul-->
		</div>    
        <!-- /.container -->
		<div class="col-xs-12 text-center abcen wow fadeIn">
			<div class="button_down "> 
				<a class="imgcircle wow bounceInUp" data-wow-duration="1.5s"  href="#scope"> <img class="img_scroll" src="img/icon/circle.png" alt=""> </a>
			</div>
		</div>
    </div>
	
	<!-- NavBar-->
	<nav class="navbar-default" role="navigation">
		<div class="container">
			<div class="navbar-header">
				<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
					<span class="sr-only">Toggle navigation</span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				</button>
				<a class="navbar-brand" href="#home">Cooperative Perception Workshop 2022</a>
			</div>

			<div class="collapse navbar-collapse navbar-right navbar-ex1-collapse">
				<ul class="nav navbar-nav">
					
					<li class="menuItem"><a href="#scope">Home</a></li>
			
					<li class="menuItem"><a href="#invited">Invited Speakers</a></li>
					<li class="menuItem"><a href="#program">Program</a></li>
					<li class="menuItem"><a href="#organizers">Organizers</a></li>	
					 <li class="menuItem"><a href="#sponsors">Sponsors</a></li>					 
					<li class="menuItem"><a href="#contacts">Contacts</a></li>
			
				
				</ul>
			</div>
		   
		</div>
	</nav> 
	
	<!-- Scope -->
    <div id ="scope" class="content-section-a" style="border-top: 0">

        <div class="container">
			
            <div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInRightBig" data-animation-delay="200">   
                    <h3 class="section-heading">1<sup>st</sup> Cooperative Perception Workshop (MFI 2022)</h3>
					<div class="sub-title lead" style="text-align:center"> Sponsored by:</div>
                     <p class="lead"  style="text-align:center"> 
						 <a href="https://en.westwell-lab.com/index.html" target="_blank"><img src="img/sponsor/westwell.png" height="50"></a> 
						 &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;
						 <a href="https://zhidx.com" target="_blank"><img src="img/sponsor/zhidongxi2.png" height="60"></a> 
					 </p> 

		    </p>
                    <p class="lead"  style="text-align:justify">
						Single-vehicle perception systems tend to suffer from occlusion and sparse sensor observation at a far distance, which can potentially lead to failure to detect objects and cause catastrophic consequences such as collisions. 
						By utilizing Vehicle-to-Everything (V2X) communication technology, nearby vehicles and roadside infrastructure can share visual information (e.g.,  raw sensory information, deep learning features, and detection outputs) to obtain multiple viewpoints of the same scene, leading to more accurate object detection and thus, better complete scene understanding.  
						Such a perception system with cooperation and connectivity enabled is called Cooperative Perception.
						<br/><br/>
						The potential of cooperative perception to revolutionize the automotive industry has attracted many researchers and companies. This workshop aims to spotlight the most recent progress of cooperative perception in both the academic and industrial fields.  
						
						<b>Major topics </b> covered are: <br/><br/>
						<ul class="lead">
							<li>Cooperative Perception, Planning, and Control algorithms</li>
							<li>Simulation Frameworks for Cooperative Perception</li>
							<li>Industrial standardizations</li>
							<li>Solutions to GPS error and communication delays</li>
							<li>Unsupervised learning in Cooperative Perception</li>
							<li>Datasets and Benchmarks for Cooperative Perception</li>
						</ul>
					</p>

					

					 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a> 
					 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
				</div>   
            </div>
        </div>
        <!-- /.container -->
    </div>


	<!-- Invited Speakers -->
    <div id ="invited" class="content-section-b">

        <div class="container">
			
            <div class="row">
				<div class="container">
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<h3 class="section-heading">Invited Speakers</h3>

					<div class="row wow fadeInRightBig">
						<p>  <wbr> </p>
					</div>
					<div class="row wow fadeInRightBig"  data-animation-delay="200">
						<div class="col-sm-3 wow"  data-animation-delay="200">   
							<h4 class="section-heading"><center>Andreas Festag</a></center></h4>
							<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/andres.jpg" alt=""></center>
						</div>
						<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
							<p> Dr. Andreas Festag is a professor at Technische Hochschule Ingolstadt and with the research and test center for vehicle safety CARISSMA. He is also deputy head at the Fraunhofer Application Center »Connected Mobility and Infrastructure«. Andreas has worked on various research projects for wireless and mobile communication networks and published more than 100 papers in journals, conference proceedings and workshops. His research is concerned with architecture, design and performance evaluation of wireless and mobile communication systems and protocols, with a focus on vehicular communication and Intelligent Transportation Systems (ITS). He is senior member of IEEE. Andreas Festag received his Ph.D. (2003) in Electrical Engineering from the Technical University Berlin. As researcher, he worked with the Telecommunication Networks Group (TKN) at Technical University Berlin, Heinrich-Hertz-Institute (HHI) in Berlin, NEC Laboratories in Heidelberg, Vodafone chair Mobile Communication Systems at Technical University Dresden and Fraunhofer Institute for Transportation and Infrastructure Systems (IVI).
							</p>
						</div> 
					</div>		


					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center>Jiaqi Ma </a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/JiaqiMa.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
						<p>Dr. Jiaqi Ma is an Associate Professor at the UCLA Samueli School of Engineering and Associate Director of UCLA Institute of Transportation Studies. Before that, he was Assistant/Associate Professor and Academic Director of the University of Cincinnati Advanced Transportation Collaborative, Project Manager and Research Scientist with Leidos.  He has led and managed many research projects covering areas of smart transportation systems, such as vehicle-highway automation, Intelligent Transportation Systems (ITS), connected vehicles and shared mobility. He is an Associate Editor of the IEEE Open Journal of Intelligent Transportation Systems and Journal of Intelligent Transportation Systems. He is  Member of the Transportation Research Board (TRB) Standing Committee on Vehicle-Highway Automation, Member of TRB Standing Committee on Artificial Intelligence and Advanced Computing Applications, Member of American Society of Civil Engineers (ASCE) Connected & Autonomous Vehicles Impacts Committee, Co-Chair of the IEEE ITS Society Technical Committee on Smart Mobility and Transportation 5.0.
						</p>
				</div>
				</div>

				<div class="row wow fadeInRightBig">
					<p>  <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center>Chen Feng</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/chenfeng.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
						<p> Dr. Chen Feng is an assistant professor at NYU, appointed across departments including civil and mechanical engineering, and computer science. His lab AI4CE (pronounced as A-I-force) aims to advance robot vision and machine learning through multidisciplinary use-inspired research that originates from engineering domains. Before NYU, Chen was a research scientist in the computer vision group at Mitsubishi Electric Research Labs (MERL) in Cambridge, MA, focusing on localization, mapping, and deep learning for self-driving cars and robotics. Chen holds a Bachelor's degree in geospatial engineering from Wuhan University in China, and a master’s degree in electrical engineering and a Ph.D. in civil engineering, both from the University of Michigan at Ann Arbor. Chen publishes in and reviews for prestigious AI/Robotics venues like CVPR/ICCV/ICRA/IROS, and he also serves as an associate editor for IEEE Robotics and Automation Letters (RA-L). More information on his research can be found at https://ai4ce.github.io/.
						</p>
					</div> 
				</div>		
				
				<div class="row wow fadeInRightBig">
					<p> <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center>Siheng Chen</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/sihengchen.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
						<p>Dr. Siheng Chen is a tenure-track associate professor at Shanghai Jiao Tong University. He was a research scientist at Mitsubishi Electric Research Laboratories (MERL), and an autonomy engineer at Uber Advanced Technologies Group (ATG), working on the perception and prediction systems of self-driving cars. Dr. Chen received his doctorate from Carnegie Mellon University. His work on sampling theory of graph data received the 2018 IEEE Signal Processing Society Young Author Best Paper Award. His co-authored paper on structural health monitoring received ASME SHM/NDE 2020 Best Journal Paper Runner-Up Award and another paper on 3D point cloud processing received the Best Student Paper Award at 2018 IEEE Global Conference on Signal and Information Processing. Dr. Chen contributed to the project of scene-aware interaction, winning MERL President's Award. His research interests include graph machine learning, autonomous driving and collective intelligence. 
						</p>
					</div> 

					<div class="row wow fadeInRightBig">
						<p> <wbr> </p>
					</div>
					<div class="row wow fadeInRightBig"  data-animation-delay="200">
						<div class="col-sm-3 wow"  data-animation-delay="200">   
							<h4 class="section-heading"><center>Ehsan Hashemi</a></center></h4>
							<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/Ehasn.jpg" alt=""></center>
						</div>
						<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
							<p>Dr. Ehsan Hashemi received his PhD in Mechanical and Mechatronics Engineering in 2017 from University of Waterloo, ON, Canada, and is currently an Assistant Professor at the University of Alberta and director of the Networked Optimization, Diagnosis, and Estimation (NODE) lab. Previously, he was a visiting professor at the School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology in 2019, research fellow at Karlsruhe Institute of Technology, and a postdoctoral fellow at the University of Waterloo (2017-2018). His research is focused on distributed and fault-tolerant control, automated driving systems, and cooperative intelligent transport systems. 
							</p>
						</div> 
					
				
				<div class="row wow fadeInRightBig">
						<p>  <wbr> </p>
					</div>
					<div class="row wow fadeInRightBig"  data-animation-delay="200">
						<div class="col-sm-3 wow"  data-animation-delay="200">   
							<h4 class="section-heading"><center>Hang Qiu</a></center></h4>
							<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/qiuhang.jpg" alt=""></center>
						</div>
						<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
							<p>Dr. Hang Qiu is software engineer at Waymo. Previously, he was a postdoctoral scholar in the Platform Lab at Stanford University, advised by Prof. Sachin Katti. He received his Ph.D. from the Department of Electrical and Computer Engineering at the University of Southern California, advised by Prof. Ramesh Govindan. His research focus is on cooperative intelligence in networked autonomous robots and cyber-physical systems with edge ML. He received a Bachelor's degree from Shanghai Jiao Tong University. He is a USC Annenberg Fellow, a Qualcomm Innovation Fellowship Finalist, an Outstanding Winner of COMAP ICM. His research is recognized by several awards, including ACM Mobisys Best Paper Runner-up Awar, and MLSys Outstanding Paper Award.
							</p>
						</div> 
				</div>		

				<div class="row wow fadeInRightBig">
					<p>  <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center>Rongqi Gu </a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/guo.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
						<p>Dr. Rongqi GU is the partner and the Director of Perception Team in Westwell. He received Master's degree from Univeristy of Illinois at Urbana-Champaign and Bachelor's degree from Zhejiang University. He is getting Ph.D. from Tongji University, advise by Prof. Guang Chen. His research foucuses on the perception ,localization and deep learning for automatic vehicles. Westwell commits to activate multi-industry potential with artificial intelligence and trys to be a pioneer in the global intelligent service industry. With artificial intelligence technology and automatic vehicle as the anchor point, Westwell will drive the intelligent service of production factors in the global logistics field, as well as the transformation and upgrading of digital and intelligent operation of urban living factors.
						</p>
					</div> 
			</div>		

				</div>				
           	 </div>
        	</div>
    	</div>
	</div>
	
	<!-- Program -->
    <div id ="program" class="content-section-a">

        <div class="container">				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading">Program</h3>
                    <p class="lead"  style="text-align:justify;font-size:15px">
						
                    	<!-- Room: Seaside 7 -->

			    <b> N.B.</b>  Time is London Time pm; Every session has 20 minutes presentation and 5 minutes Q&A.
				<p class="lead"  style="text-align:justify">
					<b>Registration Link (Free)</b>: https://www.mfi2022.com/registration<br>
					<b>Zoom Link</b>: https://cranfield.zoom.us/j/85476390238?pwd=M2J2SDU2OE1DUDUrUlE1TDJxQXhhUT09 <br>
					
					</p>
			    
<!--
			    <p class="lead"  style="text-align:justify">  Full recording of the event is available at <u>   <a href="https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang" >https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang</a>  </u>
			    </p>
-->

					<!-- <p class="lead"  style="text-align:justify">	
					Schedule is being decided <br>
					</p> -->

					<p class="lead"  style="text-align:justify">
					01:05-01:30 - Keynote 1 - <b> Siheng Chen</b>  - <i> "Where2comm: Communication-efficient Collaborative Perception via Spatial Confidence Maps" </i>  <br>.
					
					</p>

					<p class="lead"  style="text-align:justify;font-size:15px;padding-left: 5em">
					  <i> Abstract: </i> Multi-agent collaborative perception could significantly upgrade the perception performance by enabling agents to share complementary information with each other through communication. It inevitably results in a fundamental trade-off between perception performance and communication bandwidth. To tackle this bottleneck issue, we propose a spatial confidence map, which reflects the spatial heterogeneity of perceptual information. It empowers agents to only share spatially sparse, yet perceptually critical information, contributing to where to communicate. Based on this novel spatial confidence map, we propose Where2comm, a multi-round multi-agent collaborative perception framework. Where2comm has two distinct advantages: i) it uses less communication to achieve higher perception performance by focusing on perceptually critical areas; and ii) it can handle varying communication bandwidth by dynamically adjusting spatial areas involved in communication. To evaluate Where2comm, we consider 3D object detection tasks with two modalities (camera/LiDAR) and two agent types (cars/drones) on three datasets: OPV2V, V2X-Sim, and our original CoPerception-UAVs. Where2comm consistently achieves significant improvements over previous methods; for example, it achieves more than 10,000 times lower communication volume and still outperforms DiscoNet and V2VNet on OPV2V. <br>
					</p>

					<p class="lead"  style="text-align:justify">
						01:30-01:55 - Keynote 2 - <b> Andreas Festag </b>  - <i> "How to realize cooperative perception as a V2X communication service?" </i> <br>
						</p>
						
						<p class="lead"  style="text-align:justify;font-size:15px;padding-left: 5em">
							<i> Abstract: </i>Sensor data sharing is commonly regarded as a fundamental service of vehicle-2-X (V2X) communication systems that enables various safety and traffic efficiency applications. It is currently standardized as a next generation V2X communication service that is supposed to seamlessly integrate into the other services. The talk gives an overview of cooperative perception from the (European) standardization perspectives and discusses selected challenges, such as message generation, message format, object inclusion and network congestion. <br>

							<br>
							</p>
					    

					<p class="lead"  style="text-align:justify">
					01:55-02:20 - Keynote 3 - <b>Chen Feng</b>  - <i> "Towards Task-Agnostic Collaborative Perception" </i> <br>
					</p>
					
					<p class="lead"  style="text-align:justify;font-size:15px;padding-left: 5em">
						<i> Abstract: </i>Collaborative perception learns how to share information among multiple robots to perceive the environment better than individually done. Past research on this has been task-specific, such as object detection and semantic segmentation. However, this may lead to different information sharing for different tasks, which could hinder the large-scale deployment of collaborative perception. We propose the first task-agnostic collaborative perception paradigm that learns a single collaboration module in a self-supervised manner for different downstream tasks. This is done by a novel task termed collaborative scene completion, where each individual robot learns to effectively share information for reconstructing a complete scene viewed by all robots. Moreover, we propose a spatial-temporal-aware autoencoder that amortizes over time the communication cost by spatial sub-sampling and temporal mixing when sharing information. We conduct extensive experiments with various baselines to validate our method's effectiveness on scene completion and collaborative perception tasks in autonomous driving. <br>
						</p>
						
					
					<p class="lead"  style="text-align:justify">
						02:20-02:45 - Keynote 4 - <b> Jiaqi Ma </b>  - <i> "OpenCDA: An Open Cooperative Driving Automation Research Framework Integrated with Co-Simulation" </i> <br>
						</p>
						<p class="lead"  style="text-align:justify;font-size:15px;padding-left: 5em">
							<i> Abstract:</i> Although Cooperative Driving Automation (CDA) has attracted considerable attention in recent years, there remain numerous open challenges in this field. The gap between existing simulation platforms that mainly concentrate on single-vehicle intelligence and CDA development is one of the critical barriers, as it inhibits researchers from validating and comparing different CDA algorithms conveniently. In this presentation, we want to talk about our major work that fills this gap, OpenCDA. OpenCDA is a comprehensive research framework that integrates multiple popular simulator and supports the develpment of cooperative driving functinos, including cooperative perception, planning, localization, and control. We will also introduce two cooperative perception works powered by OpenCDA -- OPV2V and V2X-ViT, which is accepted by ICRA and ECCV.</i> <br>
						  </p>
							  
					
					<p class="lead"  style="text-align:justify">
						02:45-03:10 - Keynote 5 - <b> Ehsan Hashemi </b>  - <i> "Connected Autonomous Driving: Towards Reliable Perception and Resilient Control Systems" </i> <br>
						</p>
						
						<p class="lead"  style="text-align:justify;font-size:15px;padding-left: 5em">
							<i> Abstract: </i> Advances in applications of cooperative control and perception in networked autonomous systems, such as intelligent transport, facilitate reliable navigation, and robust control of fully/partially autonomous vehicles. In this regard, computationally efficient architecture for cooperative perception and state estimation, among different automated driving systems (ADS) in the vehicular network, plays a key role for development of a resilient control system, enhancing reliability of perception, state estimation, and motion planning in ADS. In the first part of presentation, a game-theory-based cooperative control framework for automated driving systems, in which guidance, path tracking, and corner traction control strategies are formulated in terms of players in a differential game, will be presented. Then, an integrated stabilization and traction ADS control framework, which considers the combined-slip friction effect, and its extension to a combined longitudinal and lateral controls for the vehicle leader-follower problem, will be discussed in the second part of the presentation. Road experiments confirmed the validity and robustness of the state estimator and controller, in different driving scenarios, especially for combined-slip and low-excitation maneuvers, which are demanding for existing control systems in ADS and advanced driver-assistance systems. <br>

							<br>
							</p>
										
									
					<p class="lead"  style="text-align:justify">
						02:10-03:35 - Keynote 6 - <b> Hang Qiu </b>  - <i> "AutoCast: scalable infrastructure-less cooperative perception for distributed collaborative driving" </i> <br>
						</p>
						
						<p class="lead"  style="text-align:justify;font-size:15px;padding-left: 5em">
							<i> Abstract: </i> Autonomous vehicles use 3D sensors for perception. Cooperative perception enables vehicles to share sensor readings with each other to improve safety. Prior work in cooperative perception scales poorly even with infrastructure support. AUTOCAST1 enables scalable infrastructure-less cooperative perception using direct vehicle-to-vehicle communication. It carefully determines which objects to share based on positional relationships between traffic participants, and the time evolution of their trajectories. It coordinates vehicles and optimally schedules transmissions in a distributed fashion. Extensive evaluation results under different scenarios show that, unlike competing approaches, AUTOCAST can avoid crashes and near-misses which occur frequently without cooperative perception, its performance scales gracefully in dense traffic scenarios providing 2-4x visibility into safety critical objects compared to existing cooperative perception schemes, its transmission schedules can be completed on the real radio testbed, and its scheduling algorithm is near-optimal with negligible computation overhead. <br>

							<br>
							</p>


					<p class="lead"  style="text-align:justify">
						03:35-04:00 - Keynote 7 - <b>Rongqi GU </b>  - <i> "Simulation framework for autonomous driving" </i> <br>
						</p>
						
						<p class="lead"  style="text-align:justify;font-size:15px;padding-left: 5em">
							<i> Abstract: </i>The vehicle simulation platform has a process simulation based on vehicle physical model and traffic simulation.
							By constructing analog camera and laser sensors in virtual environment, the perception results occurred during the operation of the vehicle in the simulation scene. This system will help verify autonomous driving perception, decision -making, planning and control algorithm.. <br>

							<br>
							</p>
												
							
					
				
				</div>
            </div>
            <div class="container">				
                <div class="wow fadeInRightBig" data-animation-delay="200">
                </div>
            </div>
        </div>
    </div>

	<!-- Organizers -->
    <div id="organizers" class="content-section-a"> 
		
		<div class="container">
		        <div class="row wow fadeInRightBig"  data-animation-delay="200">
				<h3 class="section-heading">Organizers</h3>
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Jiaqi Ma</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/Jiaqi.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of California,Los Angeles</i></center></h5>
				</div>
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="">   
					<h4 class="section-heading"><center>Runsheng Xu</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/runsheng.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of California,Los Angeles</i></center></h5>
				</div>
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Guang Chen</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/invited/chenchen.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Tongji University </i></center></h5>
				</div>
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Xin Xia</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/Xin.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of California,Los Angeles</i></center></h5>
				</div>

				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Chen Chao</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/chenchao2.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Westwell Technology</i></center></h5>
				</div>
				
				</div>
				
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
				<!--
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Zeynep Akata</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/akata.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of Tübingen, Germany </i></center></h5>
				</div>
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Bodo Rosenhahn</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/rosenhahn.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Institut für Informationsverarbeitung, Leibniz-Universität Hannover, Germany</i></center></h5>
				</div>
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Vittorio Murino</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/murino.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Istituto Italiano di Tecnologia & Università di Verona, Italy </i></center></h5>
				</div>
				-->
			</div>
        </div>
    </div>

    <!-- Sponsor -->
     <div id ="sponsors" class="content-section-a" style="border-top: 0"> 
         <div class="container">			 
             <div class="row">			 
				<div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div				
                 <div class="wow fadeInRightBig" data-animation-delay="200">    
                     <h3 class="section-heading"> Sponsors </h3> 
					<div class="sub-title lead">We gratefully acknowlegde our sponsors</div>
					<p> </p>
                     <p class="lead"  style="text-align:justify"> 
						 <a href="https://en.westwell-lab.com/index.html" target="_blank"><img src="img/sponsor/westwell.png" width="400"></a> 
					 </p> 
					 <p> <a class="btn btn-embossed btn-info" href="https://en.westwell-lab.com/index.html" role="button">Visit Website</a></p
					<p> &nbsp;&nbsp;</p>
					 <p class="lead"  style="text-align:justify"> 
						 <a href="https://zhidx.com/" target="_blank"><img src="img/sponsor/zhidongxi2.png" width="300"></a> 
					 </p> 
					 <p> <a class="btn btn-embossed btn-info" href="https://zhidx.com/" role="button">Visit Website</a></p
				 </div>    
             </div> 
         </div> 
     </div> 

	<div id="contacts" class="content-section-c ">
			 <!-- Contacts -->
		<div class="container">
			<div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading" style="color:white">Contacts</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <p class="lead"  style="text-align:left;color:white">
						For additional info please contact us <u><a style="color:white" href="mailto:rxx3386@ucla.edu">here</a></u> 
					</p>
					
				</div>   
				<!-- <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading"></h3>
                    <p class="lead"  style="text-align:right">
						©MULA2019
					</p>
					
				</div>   	 -->				
            </div>
			<div class="row">
			
						<div class="col-md-6 col-md-offset-3 text-center">
							<div >
									<div class="morph-button ">
										<button type="button"></button>
										
									</div>
							</div>
						</div>	
			</div>
		</div>
	</div>	

   
    <footer>
    
    </footer>

    <!-- JavaScript -->
    <script src="js/jquery-1.10.2.js"></script>
    <script src="js/bootstrap.js"></script>
	<script src="js/owl.carousel.js"></script>
	<script src="js/script.js"></script>
	<!-- StikyMenu -->
	<script src="js/stickUp.min.js"></script>
	<script type="text/javascript">
	  jQuery(function($) {
		$(document).ready( function() {
		  $('.navbar-default').stickUp();
		  
		});
	  });
	
	</script>
	<!-- Smoothscroll -->
	<script type="text/javascript" src="js/jquery.corner.js"></script> 
	<script src="js/wow.min.js"></script>
	<script>
	 new WOW().init();
	</script>
	<script src="js/classie.js"></script>
	<script src="js/uiMorphingButton_inflow.js"></script>
	<!-- Magnific Popup core JS file -->
	<script src="js/jquery.magnific-popup.js"></script> 
</body>

</html>
