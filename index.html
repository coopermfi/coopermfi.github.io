 <!-- FlatFy Theme - Andrea Galanti /-->
<!doctype html>
<!--[if lt IE 7]> <html class="no-js ie6 oldie" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js ie7 oldie" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js ie8 oldie" lang="en"> <![endif]-->
<!--[if IE 9]>    <html class="no-js ie9" lang="en"> <![endif]-->
<!--[if gt IE 9]><!--> <html> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0">
    <meta name="description" content="First Cooperative Perception Workshop  ">
    <meta name="author" content="">

    <title>Cooperative Perception Workshop 2022</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
 
    <!-- Custom Google Web Font -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Lato:100,300,400,700,900,100italic,300italic,400italic,700italic,900italic' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Arvo:400,700' rel='stylesheet' type='text/css'>
	
    <!-- Custom CSS-->
    <link href="css/general.css" rel="stylesheet">
	
	 <!-- Owl-Carousel -->
    <link href="css/custom.css" rel="stylesheet">
	<link href="css/owl.carousel.css" rel="stylesheet">
    <link href="css/owl.theme.css" rel="stylesheet">
	<link href="css/style.css" rel="stylesheet">
	<link href="css/animate.css" rel="stylesheet">
	
	<!-- Magnific Popup core CSS file -->
	<link rel="stylesheet" href="css/magnific-popup.css"> 
	
	<script src="js/modernizr-2.8.3.min.js"></script>  <!-- Modernizr /-->
	<!--[if IE 9]>
		<script src="js/PIE_IE9.js"></script>
	<![endif]-->
	<!--[if lt IE 9]>
		<script src="js/PIE_IE678.js"></script>
	<![endif]-->

	<!--[if lt IE 9]>
		<script src="js/html5shiv.js"></script>
	<![endif]-->

</head>

<body id="home">

	<!-- Preloader -->
	<div id="preloader">
		<div id="status"></div>
	</div>
	
	<!-- FullScreen -->
    <div class="intro-header">
		<div class="col-xs-12 text-center abcen1">
			<h1 class="h1_home wow fadeIn" data-wow-delay="0.4s" style="text-shadow: 0 0 8px #000000;">1<sup>st</sup>  Cooperative Perception Workshop </h1>
			<h3 class="h3_home wow fadeIn" data-wow-delay="0.6s" style="text-shadow: 0px 0px 4px black, 0 0 25px black"> 
				<br></br>
				<br></br>
				<p>In conjunction with <a style="color:white" href="https://www.mfi2022.com/" target="_blank"><b>mfi 2022</b></a>. </p> September 22<sup>nd</sup> 2022 (Afternoon)</p>
				<!-- <p>Room: Seaside 7</p> -->
			</h3>
			<!--ul class="list-inline intro-social-buttons">
				<li><a href="https://twitter.com/galantiandrea" class="btn  btn-lg mybutton_cyano wow fadeIn" data-wow-delay="0.8s"><span class="network-name">Twitter</span></a>
				</li>
				<li id="download" ><a href="#downloadlink" class="btn  btn-lg mybutton_standard wow swing wow fadeIn" data-wow-delay="1.2s"><span class="network-name">Free Download</span></a>
				</li>
			</ul-->
		</div>    
        <!-- /.container -->
		<div class="col-xs-12 text-center abcen wow fadeIn">
			<div class="button_down "> 
				<a class="imgcircle wow bounceInUp" data-wow-duration="1.5s"  href="#scope"> <img class="img_scroll" src="img/icon/circle.png" alt=""> </a>
			</div>
		</div>
    </div>
	
	<!-- NavBar-->
	<nav class="navbar-default" role="navigation">
		<div class="container">
			<div class="navbar-header">
				<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
					<span class="sr-only">Toggle navigation</span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				</button>
				<a class="navbar-brand" href="#home">Cooperative Perception Workshop 2022</a>
			</div>

			<div class="collapse navbar-collapse navbar-right navbar-ex1-collapse">
				<ul class="nav navbar-nav">
					
					<li class="menuItem"><a href="#scope">Home</a></li>
			
					<li class="menuItem"><a href="#program">Program</a></li>
					<li class="menuItem"><a href="#invited">Invited Speakers</a></li>
					<li class="menuItem"><a href="#organizers">Organizers</a></li>	
					 <li class="menuItem"><a href="#sponsors">Sponsors</a></li>					 
					<li class="menuItem"><a href="#contacts">Contacts</a></li>
			
				
				</ul>
			</div>
		   
		</div>
	</nav> 
	
	<!-- Scope -->
    <div id ="scope" class="content-section-a" style="border-top: 0">

        <div class="container">
			
            <div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInRightBig" data-animation-delay="200">   
                    <h3 class="section-heading">1<sup>st</sup> Cooperative Perception Workshop (MFI 2022)</h3>
					<div class="sub-title lead" style="text-align:center"> Sponsored by:</div>
                     <p class="lead"  style="text-align:center"> 
						 <a href="https://en.westwell-lab.com/index.html" target="_blank"><img src="img/sponsor/westwell.png" height="70"></a> 
						 &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;
						 <a href="https://zhidx.com" target="_blank"><img src="img/sponsor/zhidongxi.png" height="80"></a> 
					 </p> 

		    </p>
                    <p class="lead"  style="text-align:justify">
						Single-vehicle perception systems tend to suffer from occlusion and sparse sensor observation at a far distance, which can potentially lead to failure to detect objects and cause catastrophic consequences such as collisions. <br/><br/>
						By utilizing Vehicle-to-Everything (V2X) communication technology, nearby vehicles and roadside infrastructure can share visual information (e.g.,  raw sensory information, deep learning features, and detection outputs) to obtain multiple viewpoints of the same scene, leading to more accurate object detection and thus, better complete scene understanding.  <br/><br/>
						Such a perception system with cooperation and connectivity enabled is called Cooperative Perception.
						<br/><br/>
						The potential of cooperative perception to revolutionize the automotive industry has attracted many researchers and companies. This workshop aims to spotlight the most recent progress of cooperative perception in both the academic and industrial fields.  <br/><br/>
						
						<b>Major topics </b> covered are: <br/><br/>
						<ul class="lead">
							<li>Datasets and benchmarks</li>
							<li>Simulation Frameworks</li>
							<li>Efficient fusion algorithms</li>
							<li>Solutions to GPS error and communication delays</li>
							<li>Industrial standardizations</li>
						</ul>
					</p>

					

					 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a> 
					 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
				</div>   
            </div>
        </div>
        <!-- /.container -->
    </div>


	
	<!-- Program -->
    <div id ="program" class="content-section-a">

        <div class="container">				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading">Program</h3>
                    <p class="lead"  style="text-align:justify;font-size:15px">
						
                    	<!-- Room: Seaside 7 -->

			    <b> N.B.</b>  Time is London Time;
			    
<!--
			    <p class="lead"  style="text-align:justify">  Full recording of the event is available at <u>   <a href="https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang" >https://www.youtube.com/watch?v=pHuFMcaoLio&ab_channel=MichaelYang</a>  </u>
			    </p>
-->

					<p class="lead"  style="text-align:justify">	
					08:30-08:40 - Welcome from organizers and openings remarks <br>
					</p>
					<p class="lead"  style="text-align:justify">
					08:40-09:10 - Keynote 1 - <b> Cordelia  Schmid</b>  - <i> "Large-scale learning from multimodal videos" </i>  <br>.
					
					</p>
<!--
					<p class="lead"  style="text-align:justify;font-size:15px;padding-left: 5em">
					  <i> Abstract: </i>  The tremendous growth of multimodal video data in recent years has increased the demand for efficient multimodal deep neural network models, particularly in domains where real-time inference is essential. While significant progress has been made on model compression and acceleration for video understanding, most existing methods rely on one-size-fits-all models, which apply the same amount of computation for all video segments across all modalities. In this talk, I will instead cover methods that adaptively change computation depending on the content of the input. In particular, in the context of audio-visual action recognition, I will describe a method that adaptively decides which modality to use for each video segment (deciding where to look at and listen to in the video), with the goal of improving both accuracy and efficiency. Finally, I will conclude my talk by describing ongoing work that integrates this technology into a system for auto-curation of sports highlights based on multimodal video understanding. <br>
					</p>
-->
					<p class="lead"  style="text-align:justify">
					09:10-10:00 - Oral Session 1 (5-min presentations + 2-min Q&A - <u>  <a href="https://es.sonicurlprotection-fra.com/click?PV=2&MSGID=202206150750140119783&URLID=5&ESV=10.0.16.7295&IV=54F9BCC591AEB2404663640C364FB356&TT=1655279416266&ESN=AUW6yS1kctQDvbCeZBi%2FJleBAs3dZyW6bkos6LMkyUE%3D&KV=1536961729280&B64_ENCODED_URL=aHR0cHM6Ly9mb3Jtcy5nbGUvYmE5QVV6SHc5d0Z0UVI0SkE&HK=045A7D12BFFD1042A7FF528D9036830A87800DB6736685A6B10083417BA1F1E1"> form link for asynchronous Q&A</a></u>) <br>
					</p>
					<p class="lead" style="text-align:justify;font-size:15px;padding-left: 5em">
						(ID 03) - Transformer Decoders with MultiModal Regularization for Cross-Modal Food Retrieval - <i> Mustafa Shukor, Guillaume Couairon; Asya Grechka Matthieu Cord. </i> <u><a href="https://drive.google.com/file/d/1Nc79zo6ElkvI8LX359PsuBpYBn9yHG8-/view?usp=sharing">link</a> </u><br> 
						(ID 05) - Improving Multimodal Speech Recognition by Data Augmentation and Speech Representations - <i> Dan Oneață, Horia Cucu.</i> <u><a href="https://drive.google.com/file/d/1AJvULaA7qSdcQuuhKj5gBul20C4dGoZ0/view?usp=sharing">link</a> </u><br>
						(ID 19) - Coupling Vision and Proprioception for Navigation of Legged Robots - <i> Zipeng Fu, Ashish Kumar, Ananye Agarwal, Haozhi Qi, Jitendra Malik, Deepak Pathak.</i> <u><a href="https://drive.google.com/file/d/1icMaeJt6dAFkbEgvgbQ6UB9IUrV8MRh3/view?usp=sharing">link</a> </u><br>
						(ID 27) - M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation - <i> Vishal M. Chudasama, Purbayan Kar, Ashish Gudmalwar, Nirmesh J. Shah, Pankaj Wasnik, Naoyuki Onoe.</i> <u><a href="https://drive.google.com/file/d/1vPCsxPrN-4wL89bLkAKCkyzWS63kxB-H/view?usp=sharing">link</a> </u><br>
						(ID 34) - Cascaded Siamese Self-Supervised Audio to Video GAN - <i> Nuha N Aldausari, Arcot Sowmya, Nadine Marcus, Gelareh Mohammadi.</i> <u><a href="https://drive.google.com/file/d/1FIF_m232-vbGBsY3GBmXrbWPLR3Ohoiv/view?usp=sharing">link</a> </u><br>
						(ID 38) - Multi-view Multi-label Canonical Correlation Analysis for Cross-modal Matching and Retrieval - <i> Rushil Kaushal Sanghavi, Yashaswi Verma.</i> <u><a href="https://drive.google.com/file/d/1vIWNhiGuHDXsq8_jM0AOfTKD_2gfDlFS/view?usp=sharing">link</a> </u><br>				    
					</p>
					    
					<p class="lead"  style="text-align:justify">
					10:00-10:30 - Keynote 2 - <b> Kate  Saenko </b>  - <i> "More Language, Less Labeling: Vision and Language Pretraining for Visual Tasks" </i> <br>
					</p>
					
					<p class="lead"  style="text-align:justify">
					10:30-11:00 - Coffee break <br>
					</p>
					
					<p class="lead"  style="text-align:justify">
					11:00-12:15 - Oral Session 2  (5-min presentations + 2-min Q&A - <u>  <a href="https://es.sonicurlprotection-fra.com/click?PV=2&MSGID=202206150750140119783&URLID=5&ESV=10.0.16.7295&IV=54F9BCC591AEB2404663640C364FB356&TT=1655279416266&ESN=AUW6yS1kctQDvbCeZBi%2FJleBAs3dZyW6bkos6LMkyUE%3D&KV=1536961729280&B64_ENCODED_URL=aHR0cHM6Ly9mb3Jtcy5nbGUvYmE5QVV6SHc5d0Z0UVI0SkE&HK=045A7D12BFFD1042A7FF528D9036830A87800DB6736685A6B10083417BA1F1E1"> form link for asynchronous Q&A</a></u>) <br>
					</p>
					<p class="lead" style="text-align:justify;font-size:15px;padding-left: 5em">
						(ID 01) - Probabilistic Compositional Embeddings for Multimodal Image Retrieval - <i>Andrei Neculai, Yanbei Chen, Zeynep Akata.</i> <u><a href="https://drive.google.com/file/d/1DMD4auDLCTHrglpA8cuqgB6RQ_SbwB7h/view?usp=sharing">link</a> </u><br>
					    (ID 02) - Coarse-to-Fine Reasoning for Visual Question Answering - <i> Binh Xuan Nguyen, Tuong Khanh Long Do, Huy Tran, Erman Tjiputra, Quang Duy Tran, Anh Nguyen. </i> <u><a href="https://drive.google.com/file/d/1X_6JjD7ykS6mPi-FawoKEO9FhlIILReq/view?usp=sharing">link</a> </u><br>
					    (ID 06) - Semantically Grounded Visual Embeddings for Zero-Shot Learning - <i> Shah Nawaz, Jacopo Cavazza, Alessio Del Bue.</i> <u><a href="https://drive.google.com/file/d/1-Du2iEcfJoq-wFHipNqxiYg5LuieNZHj/view?usp=sharing">link</a> </u><br>
					    (ID 11) - Reasoning with Multi-structure Commonsense Knowledge in Visual Dialog - <i> Shunyu Zhang, Xiaoze Jiang, Zequn Yang, Tao Wan, Zengchang Qin. </i> <u><a href="https://drive.google.com/file/d/1FfZqn7UJuo8U7coOsWun28eqmTMCQVkL/view?usp=sharing">link</a> </u><br>
					    (ID 16) - Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters - <i> Ilker Kesen, Ozan Can, Erkut Erdem, Aykut Erdem, Deniz Yüret. </i> <u><a href="https://drive.google.com/file/d/1O50xQ91hyRNB5vrxcf0A7-PQLOBOaag_/view?usp=sharing">link</a> </u><br>
					    (ID 20) - Emphasizing Complementary Samples for Non-literal Cross-modal Retrieval - <i> Christopher L. Thomas, Adriana Kovashka. </i> <u><a href="https://drive.google.com/file/d/1kGaj7lF8MFHreCmNh95aBhwNcbZA2Xna/view?usp=sharing">link</a> </u><br>
					    (ID 21) - Doubling down: sparse grounding with an additional, almost-matching caption for detection-oriented multimodal pretraining - <i> Giacomo Nebbia, Adriana Kovashka.</i> <u><a href="https://drive.google.com/file/d/1-ddZhLZ7GqyWQ8m_Ku3edSQHp5HBT8IO/view?usp=sharing">link</a> </u><br>
					    (ID 28) - The Unreasonable Effectiveness of CLIP features for Image Captioning: an Experimental Analysis - <i> Manuele Barraco, Marcella Cornia, Silvia Cascianelli, Lorenzo Baraldi, Rita Cucchiara. </i> <u><a href="https://drive.google.com/file/d/11QHJNrFiB_vrmNPemqJoyTEBY83ditQq/view?usp=sharing">link</a> </u><br>
					    (ID 30) - Guiding Attention using Partial-Order Relationships for Image Captioning - <i> Murad Popattia, Muhammad Rafi, Rizwan Qureshi, Shah Nawaz. </i> <u><a href="https://drive.google.com/file/d/1MgReA8qHXgBI-5OofnhKBUIxYvqM_i7y/view?usp=sharing">link</a> </u><br>
					    (ID 33) - Learning to Ask Informative Sub-Questions for Visual Question Answering - <i> Kohei Uehara, Nan Duan, Tatsuya Harada.</i> <u><a href="https://drive.google.com/file/d/14q3N4HRjXssI0ULTTOLIS3r7_keN7GbR/view?usp=sharing">link</a> </u><br>
					</p>

					<p class="lead"  style="text-align:justify">
					12:15-12:45 - Keynote 3 - <b> James Rehg </b> - <i> Learning to Navigate from Vision and Language </i> <br> 
					</p>
					<p class="lead"  style="text-align:justify">
					12:45-13:00 - Closing Remarks and Best Paper Award ceremony <br>
					</p>
					<img src="img/sponsor/snap_award.png" height="300"></a> <img src="img/sponsor/bosch_award.png" height="300"></a> 
<!--
					<p class="lead"  style="text-align:justify">
					11:35-12:00 -  Poster Session (all papers)  <br>
					</p>
-->

					
				
				</div>
            </div>
            <div class="container">				
                <div class="wow fadeInRightBig" data-animation-delay="200">
                </div>
            </div>
        </div>
    </div>
	<!-- Invited Speakers -->
    <div id ="invited" class="content-section-b">

        <div class="container">
			
            <div class="row">
				<div class="container">
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<h3 class="section-heading">Invited Speakers</h3>

				  


					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center>Jiaqi Ma </a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/JiaqiMa.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
						<p> Dr. Jiaqi Ma is an Associate Professor at the UCLA Samueli School of Engineering and Associate Director of UCLA Institute of Transportation Studies. Prior to that, he was Assistant/Associate Professor and Academic Director of the University of Cincinnati Advanced Transportation Collaborative, Project Manager and Research Scientist with Leidos working at the Federal Highway Administration Turner-Fairbank Highway Research Center.  He has led and managed many research projects worth a total value of more than $ 16 million funded by U.S. DOT, NSF, state DOTs, and other federal/state/local programs covering areas of smart transportation systems, such as vehicle-highway automation, Intelligent Transportation Systems (ITS), connected vehicles, shared mobility, and large-scale smart system modeling and simulation, and artificial intelligence and advanced computing applications in transportation. He is an Associate Editor of the IEEE Open Journal of Intelligent Transportation Systems and Journal of Intelligent Transportation Systems. He is  Member of the Transportation Research Board (TRB) Standing Committee on Vehicle-Highway Automation, Member of TRB Standing Committee on Artificial Intelligence and Advanced Computing Applications, Member of American Society of Civil Engineers (ASCE) Connected & Autonomous Vehicles Impacts Committee, Co-Chair of the IEEE ITS Society Technical Committee on Smart Mobility and Transportation 5.0.
						</p>
				</div>
				</div>

				<div class="row wow fadeInRightBig">
					<p>  <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center>Chen Feng</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/chenfeng.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
						<p> Dr. Chen Feng is an assistant professor at NYU, appointed across departments including civil and mechanical engineering, and computer science. His lab AI4CE (pronounced as A-I-force) aims to advance robot vision and machine learning through multidisciplinary use-inspired research that originates from civil/mechanical engineering domains. Before NYU, Chen was a research scientist in the computer vision group at Mitsubishi Electric Research Labs (MERL) in Cambridge, MA, focusing on localization, mapping, and deep learning for self-driving cars and robotics. Chen holds a Bachelor's degree in geospatial engineering from Wuhan University in China, and a master’s degree in electrical engineering and a Ph.D. in civil engineering, both from the University of Michigan at Ann Arbor.
						</p>
					</div> 
				</div>		
				
				<div class="row wow fadeInRightBig">
					<p> <wbr> </p>
				</div>
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
					<div class="col-sm-3 wow"  data-animation-delay="200">   
						<h4 class="section-heading"><center>Siheng Chen</a></center></h4>
						<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/sihengchen.jpg" alt=""></center>
					</div>
					<div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
						<p>Siheng Chen is a tenure-track associate professor at Shanghai Jiao Tong University. He was a research scientist at Mitsubishi Electric Research Laboratories (MERL), and an autonomy engineer at Uber Advanced Technologies Group (ATG), working on the perception and prediction systems of self-driving cars. Dr. Chen received his doctorate from Carnegie Mellon University. His work on sampling theory of graph data received the 2018 IEEE Signal Processing Society Young Author Best Paper Award. His co-authored paper on structural health monitoring received ASME SHM/NDE 2020 Best Journal Paper Runner-Up Award and another paper on 3D point cloud processing received the Best Student Paper Award at 2018 IEEE Global Conference on Signal and Information Processing. Dr. Chen contributed to the project of scene-aware interaction, winning MERL President's Award. His research interests include graph machine learning, autonomous driving and collective intelligence. 
						</p>
					</div> 

				</div>				
           	 </div>
        	</div>
    	</div>
	</div>
	
	<!-- Organizers -->
    <div id="organizers" class="content-section-a"> 
		
		<div class="container">
		        <div class="row wow fadeInRightBig"  data-animation-delay="200">
				<h3 class="section-heading">Organizers</h3>
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Paolo Rota</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/rota.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Università di Trento, Italy</i></center></h5>
				</div>
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="">   
					<h4 class="section-heading"><center>Pietro Morerio</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/morerio.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Istituto Italiano di Tecnologia, Italy</i></center></h5>
				</div>
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Massimiliano Mancini</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/mancini.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of Tübingen, Germany </i></center></h5>
				</div>
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Michael Ying Yang</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/yang.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of Twente, Netherlands</i></center></h5>
				</div>
				
				</div>
				
				<div class="row wow fadeInRightBig"  data-animation-delay="200">
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Zeynep Akata</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/akata.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>University of Tübingen, Germany </i></center></h5>
				</div>
				
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Bodo Rosenhahn</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/rosenhahn.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Institut für Informationsverarbeitung, Leibniz-Universität Hannover, Germany</i></center></h5>
				</div>
				<div class="col-sm-3 wow fadeInRightBig"  data-animation-delay="200">   
					<h4 class="section-heading"><center>Vittorio Murino</center></h4>
					<center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="200" width="200" src="img/organizer/murino.jpg" alt=""></center>
					<h5 class="section-heading"><center><i>Istituto Italiano di Tecnologia & Università di Verona, Italy </i></center></h5>
				</div>
			</div>
        </div>
    </div>

	<!-- Program Committee -->
    <div id ="committee" class="content-section-b" style="border-top: 0">
        <div class="container">			
            <div class="row">			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading"> Acknowledgments</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
					<p class="lead"  style="text-align:justify">
						We gratefully acknowledge our reviewers

						<ul class="lead">
							AJ	Piergiovanni,
							Alina	Roitberg,
							Andrea	Pilzer,
							Andrea	Zunino,
							Anelia	Angelova,
							Anil Osman	Tur,
							Arif	Mahmood,
							Carles	Ventura,
							Christoph	Reinders,
							Dario	Fontanel,
							Davide	Talon,
							Dayan	Guan,
							Enrico	Fini,
							Fabio	Cermelli,
							Giacomo	Zara,
							Gianluca	Scarpellini,
							Guanglei	Yang,
							Haidong	Zhu,
							Haoyu	Dong,
							Hari Prasanna	Das ,
							Ichraf	Lahouli,
							Jae Myung	Kim,
							Jiguo	Li,
							Karsten	Roth,
							Leonard	Salewski,
							Letitia	Parcalabescu,
							Limin	Wang,
							Mihee	Lee,
							Nicola	Dall'Asen,
							Praneet	Dutta,
							Pritish	Sahu,
							Qing	Wan,
							Rico	Jonschkowski,
							Ruggero	Ragonesi,
							Sharath	Koorathota,
							Shih-Han	Chou,
							Shyamgopal	Karthik,
							Suvarna	Kadam,
							Tal	Hakim,
							Thiago	Oliveira-Santos,
							Thomas	Hummel,
							Thomas	Theodoridis,
							Uddeshya	Upadhyay,
							Victor	Turrisi da Costa,
							Vladimir	Kniaz,
							Vladimir	Pavlovic,
							Wentong	Liao,
							Woojeong	Jin,
							Xu	Dong,
							Yanbei	Chen,
							Yoonsuck	Choe,
							Yue	Song,
							Ze	Wang.

						</ul> 

					</p>

					 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a> 
					 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
				</div>   
            </div>
        </div>
        <!-- /.container -->
    </div>

    <!-- Sponsor -->
     <div id ="sponsors" class="content-section-a" style="border-top: 0"> 
         <div class="container">			 
             <div class="row">			 
				<div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div				
                 <div class="wow fadeInRightBig" data-animation-delay="200">    
                     <h3 class="section-heading"> Sponsors </h3> 
					<div class="sub-title lead">We gratefully acknowlegde our sponsors for supporting the Best Paper Award</div>
					<p> </p>
                     <p class="lead"  style="text-align:justify"> 
						 <a href="https://www.snap.com/" target="_blank"><img src="img/sponsor/Snap-black.png" width="400"></a> 
					 </p> 
					 <p> <a class="btn btn-embossed btn-info" href="https://www.snap.com/" role="button">Visit Website</a></p
					<p> &nbsp;&nbsp;</p>
					 <p class="lead"  style="text-align:justify"> 
						 <a href="https://www.bosch.com/" target="_blank"><img src="img/sponsor/bosch.png" width="400"></a> 
					 </p> 
					 <p> <a class="btn btn-embossed btn-info" href="https://www.bosch.com/" role="button">Visit Website</a></p
				 </div>    
             </div> 
         </div> 
     </div> 

	<div id="contacts" class="content-section-c ">
			 <!-- Contacts -->
		<div class="container">
			<div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading" style="color:white">Contacts</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <p class="lead"  style="text-align:left;color:white">
						For additional info please contact us <u><a style="color:white" href="mailto:mula.workshop@gmail.com">here</a></u> 
					</p>
					
				</div>   
				<!-- <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading"></h3>
                    <p class="lead"  style="text-align:right">
						©MULA2019
					</p>
					
				</div>   	 -->				
            </div>
			<div class="row">
			
						<div class="col-md-6 col-md-offset-3 text-center">
							<div >
									<div class="morph-button ">
										<button type="button"></button>
										
									</div>
							</div>
						</div>	
			</div>
		</div>
	</div>	

   
    <footer>
    
    </footer>

    <!-- JavaScript -->
    <script src="js/jquery-1.10.2.js"></script>
    <script src="js/bootstrap.js"></script>
	<script src="js/owl.carousel.js"></script>
	<script src="js/script.js"></script>
	<!-- StikyMenu -->
	<script src="js/stickUp.min.js"></script>
	<script type="text/javascript">
	  jQuery(function($) {
		$(document).ready( function() {
		  $('.navbar-default').stickUp();
		  
		});
	  });
	
	</script>
	<!-- Smoothscroll -->
	<script type="text/javascript" src="js/jquery.corner.js"></script> 
	<script src="js/wow.min.js"></script>
	<script>
	 new WOW().init();
	</script>
	<script src="js/classie.js"></script>
	<script src="js/uiMorphingButton_inflow.js"></script>
	<!-- Magnific Popup core JS file -->
	<script src="js/jquery.magnific-popup.js"></script> 
</body>

</html>
